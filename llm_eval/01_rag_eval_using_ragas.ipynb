{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2620d758",
   "metadata": {},
   "source": [
    "# Introduction to RAGAs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3e8e3",
   "metadata": {},
   "source": [
    "- RAGAs stands for Retrieval-Augmented Generation Assessment, which is a framework provide insights to RAG pipeline evaluation\n",
    "- RAGAs leverages LLMs under the hood to conduct the evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a5c526",
   "metadata": {},
   "source": [
    "## data format\n",
    "\n",
    "to use RAGAs, the data must be formatted as:\n",
    "\n",
    "- `question`: The user query that is the input of the RAG pipeline. The input. These are questions the RAG pipeline will be evaluated on\n",
    "- `contexts`: The contexts retrieved from the external knowledge source used to answer the question.\n",
    "- `ground_truth`: The ground truth answer to the question. This is the only human-annotated information. This information is only required for the metric context_recall (see Evaluation Metrics)\n",
    "\n",
    "- `answer`: The **generated** answer from the RAG pipeline. The output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e0b61",
   "metadata": {},
   "source": [
    "## evaluation metrics\n",
    "\n",
    "### retriever\n",
    "\n",
    "- `context_precision`: measures how relevant the retrieved context is to the question, **the quality of the pipeline**\n",
    "- `context_recall`: measures the retriever's ability to retrieve all necessary information\n",
    "\n",
    "### generator\n",
    "\n",
    "- `faithfulness`: measures the factual consistency to the context based on the question\n",
    "- `answer_relevancy`: measures how relevant the answer is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43048598",
   "metadata": {},
   "source": [
    "# load dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34f6e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "import requests\n",
    "import weaviate\n",
    "from datasets import Dataset\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    context_relevancy,\n",
    "    faithfulness,\n",
    ")\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "_ = load_dotenv(find_dotenv(\"../.env\"))\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f492230",
   "metadata": {},
   "source": [
    "# load data (raw text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c29daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/llm_eval/state_of_the_union.txt\")\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/langchain-ai/langchainjs/main/examples/state_of_the_union.txt\"\n",
    "    res = requests.get(url)\n",
    "\n",
    "    with open(data_path, \"w\") as f:\n",
    "        f.write(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4095f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "loader = TextLoader(data_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d02f7cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk the data\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67939539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 90 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"total {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d87a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started /Users/z/.cache/weaviate-embedded: process ID 12749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"startup\",\"default_vectorizer_module\":\"none\",\"level\":\"info\",\"msg\":\"the default vectorizer modules is set to \\\"none\\\", as a result all new schema classes without an explicit vectorizer setting, will use this vectorizer\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"action\":\"startup\",\"auto_schema_enabled\":true,\"level\":\"info\",\"msg\":\"auto schema enabled setting is set to \\\"true\\\"\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"No resource limits set, weaviate will use all available memory and CPU. To limit resources, set LIMIT_RESOURCES=true\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"level\":\"warning\",\"msg\":\"Multiple vector spaces are present, GraphQL Explore and REST API list objects endpoint module include params has been disabled as a result.\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"action\":\"grpc_startup\",\"level\":\"info\",\"msg\":\"grpc server listening at [::]:50060\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"action\":\"restapi_management\",\"level\":\"info\",\"msg\":\"Serving weaviate at http://127.0.0.1:8079\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Created shard langchain_c9631a309d5e4c30b963b06f2d7a02ad_RUZVLPjnxA2U in 3.263916ms\",\"time\":\"2024-09-12T01:41:51+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:51+08:00\",\"took\":63292}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_366a21d038ea4a7c848e12097746719c_1WTnSny4avI6 in 1.977209ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":35416}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_9b7d6c85d56d4372af559d932501ec56_ulxbIzvPRA8g in 2.295542ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":40750}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_6dd91c88e2154f25ba73a527e86459be_UlbYciyJeQGc in 4.616708ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":38041}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_69c8c12924b94a1e8b4f28369c30a5ce_IrbmhVU0wG4s in 5.417666ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":37208}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_7347bd19b8c74c239ed46361f73e6921_VBwQ1IryRV65 in 6.146333ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":38958}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_67b39abc987442eabc0e821248368adf_MI4BGFETDxj9 in 6.48175ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_4cc4c38732fd49bc9e7a91a59b1d9364_c6ivvluWjQjo in 6.593708ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_ac7f5185d53a4b70972885c947393711_yYuustjj0bpW in 6.554084ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"level\":\"info\",\"msg\":\"Completed loading shard langchain_d52ab8386be14af8987460b48aa92db0_syB648i08y0d in 6.989416ms\",\"time\":\"2024-09-12T01:41:52+08:00\"}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":1538291}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":1801208}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":1929166}\n",
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":3000,\"index_id\":\"main\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2024-09-12T01:41:52+08:00\",\"took\":1756917}\n"
     ]
    }
   ],
   "source": [
    "# index the data\n",
    "\n",
    "# Setup vector database\n",
    "client = weaviate.Client(embedded_options=EmbeddedOptions())\n",
    "\n",
    "# populate vector daatabase\n",
    "vectorstore = Weaviate.from_documents(\n",
    "    client=client, documents=chunks, embedding=OpenAIEmbeddings(), by_text=False\n",
    ")\n",
    "\n",
    "# Define vectorstore as retriever to enable semantic search\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64ea2cd",
   "metadata": {},
   "source": [
    "# establish a template & setup rag chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a1eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define llm\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define prompt template\n",
    "template = \"\"\"You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use two sentences maximum and keep the answer concise.\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# setup rag pipeline\n",
    "# 'RunnablePassthrough' allows data to be passed thru without modification\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addecec",
   "metadata": {},
   "source": [
    "# create dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79c1a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What did the president say about Justice Breyer?\",\n",
    "    \"What did the president say about Intel's CEO?\",\n",
    "    \"What did the president say about gun violence?\",\n",
    "]\n",
    "ground_truth = [\n",
    "    \"The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\",\n",
    "    \"The president said that Pat Gelsinger is ready to increase Intel's investment to $100 billion.\",\n",
    "    \"The president asked Congress to pass proven measures to reduce gun violence.\",\n",
    "]\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# inference\n",
    "for query in questions:\n",
    "    answers.append(rag_chain.invoke(query))\n",
    "    contexts.append(\n",
    "        [docs.page_content for docs in retriever.get_relevant_documents(query)]\n",
    "    )\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truth,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5827fa6",
   "metadata": {},
   "source": [
    "# evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3a10b20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d24c28390040b8adbbd71d73582483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select metrics\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "]\n",
    "\n",
    "# evalute\n",
    "result = evaluate(\n",
    "    dataset=dataset,\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90fe9cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What did the president say about Justice Breyer?</td>\n",
       "      <td>The president honored Justice Stephen Breyer f...</td>\n",
       "      <td>[Tonight, I’d like to honor someone who has de...</td>\n",
       "      <td>The president said that Justice Breyer has ded...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.812966</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What did the president say about Intel's CEO?</td>\n",
       "      <td>The president mentioned that Intel's CEO, Pat ...</td>\n",
       "      <td>[But that’s just the beginning. \\n\\nIntel’s CE...</td>\n",
       "      <td>The president said that Pat Gelsinger is ready...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.806654</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What did the president say about gun violence?</td>\n",
       "      <td>The president called for Congress to pass meas...</td>\n",
       "      <td>[And I ask Congress to pass proven measures to...</td>\n",
       "      <td>The president asked Congress to pass proven me...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.908946</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question  \\\n",
       "0  What did the president say about Justice Breyer?   \n",
       "1     What did the president say about Intel's CEO?   \n",
       "2    What did the president say about gun violence?   \n",
       "\n",
       "                                              answer  \\\n",
       "0  The president honored Justice Stephen Breyer f...   \n",
       "1  The president mentioned that Intel's CEO, Pat ...   \n",
       "2  The president called for Congress to pass meas...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Tonight, I’d like to honor someone who has de...   \n",
       "1  [But that’s just the beginning. \\n\\nIntel’s CE...   \n",
       "2  [And I ask Congress to pass proven measures to...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0  The president said that Justice Breyer has ded...           1.0   \n",
       "1  The president said that Pat Gelsinger is ready...           0.5   \n",
       "2  The president asked Congress to pass proven me...           1.0   \n",
       "\n",
       "   answer_relevancy  context_relevancy  context_recall  context_precision  \n",
       "0          0.812966           0.066667             1.0               1.00  \n",
       "1          0.806654           0.038462             1.0               1.00  \n",
       "2          0.908946           0.350000             1.0               0.75  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithRateLimitError('Rate limit exceeded for https://api.smith.langchain.com/runs/batch. HTTPError(\\'429 Client Error: Too Many Requests for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Monthly unique traces usage limit exceeded\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9c6eb",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Evaluating RAG Applications with RAGAs](https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a)\n",
    "- [RAGAs official document](https://docs.ragas.io/en/latest/getstarted/evaluation.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
