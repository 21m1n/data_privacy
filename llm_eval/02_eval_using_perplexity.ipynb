{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity for language models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduction\n",
    "\n",
    "### What is perplexity?\n",
    "\n",
    "- Perplexity is a metrics used to evaluate the performance of language models, particularly in the context of predicting the next word in a sequence based on the preceding words.\n",
    "- it quantifies how well a probability model predicts a sample and is defined mathematically as the exponentiation of the average negative log-likelihood of a sequence\n",
    "- the lower the better → the model is more confident → (in the context of LLMs, the model is more likely to have consistent output)\n",
    "\n",
    "### the formula of perplexity:\n",
    "\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{N}\\sum_{i=1}^N \\log_2 p(x_i|x_1, \\dots, x_{i-1})}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x_i$ is the current token\n",
    "- $x_1, \\dots, x_{i-1}$ are all the tokens that come before $x_ i$\n",
    "- $p(x_i|x_1, \\dots, x_{i-1})$ is the conditional probability of $x_i$ is occurring, given that we've already seen the sequence $x_1, \\dots, x_{i-1}$, it represents how likely the model thinks the token $x_i$ is to appear next, based on the preceding tokens\n",
    "- $\\log_2 p(x_i|x_1, \\dots, x_{i-1})$ is the log likelihood of the conditional probability that was previously mentioned. we use logarithms for reason like (numerical stability, converting products to sums, etc.)\n",
    "- $\\sum_{i=1}^N \\log_2 p(\\dots)$ represents the negative log-likelihood of an entire sequence of tokens.\n",
    "- $\\frac{1}{N} \\sum_{i=1}^{N} -\\log_2 p(x_i|x_1, \\dots, x_{i-1})$ is the cross-entropy between the true distribution of the data and our model's distribution\n",
    "- $2^{...}$ \"cancels out\" the base 2 logarithm used in the sum, which creates a symmetry that allows for a particular interpretation of perplexity\n",
    "\n",
    "### interpretation of perplexity\n",
    "\n",
    "perplexity can be thought of as the weighted average number of choice the model has when predicting each token. a perplexity of 10 means that the model is as confused on average as if it had to choose uniformly between 10 possibilities for each token.\n",
    "\n",
    "### perplexity vs cross-entropy\n",
    "\n",
    "since perplexity and cross-entropy are directly related, why do we still need perplexity?\n",
    "\n",
    "- **interpretability**: while cross-entropy provides a measure of the average uncertainty in predicting next word, perplexity translates this uncertainty into a more interpretable form (see above)\n",
    "- **historical precedent**: in the context of language models, perplexity has been a standard metric since the early days, hence it is easier to compare new results with older benchmarks\n",
    "- **scales**: perplexity is more sensitive to small changes in model performance, especially at lower values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to calculate perplexity\n",
    "\n",
    "- Tokenization: segment the text into tokens.\n",
    "- Log-Likelihood Calculation: for each token in the sequence, compute its log-likelihood based on the model's predictions, conditioned on the preceding tokens.\n",
    "- Average Negative Log-Likelihood: determine the mean of these log-likelihoods across all tokens.\n",
    "- Exponentiation: Finally, apply the exponential function to the average negative log-likelihood to derive the final perplexity score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example I: perplexity for GPT2\n",
    "\n",
    "for autoregressive models like GPT2, the term \"loss\" and \"negative log-likelihood\" are often used interchangeably, because:\n",
    "\n",
    "- the standard loss function used during training is the cross-entropy loss, which is calculated over the entire sequence\n",
    "- for classification problems (which next-token prediction essentially is), the cross-entropy loss is equivalent to the negative log-likelihood of the correct token\n",
    "- the loss returned by `outputs.loss` is the cross-entropy loss, which is mathematically equivalent to the average negative log-likelihood per token in the sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bf729156b24845abd8d6777f5ac201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67f78b261f4424384e130ef72253301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254bcd7241b341cd82a549bf9c6cc11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ca0705c20c477db5348aeb34cb703c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58221b63def14f56945ee2ee7a64bb53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bed99b504d44c298ffcd445d53de0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c15acb881c544b59e4009c3f8fe9e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/llm/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 0.009715268409795624\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_id = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Example text\n",
    "text = \"This is an example sentence to evaluate.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Calculate log likelihood\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    log_likelihood = -outputs.loss.item()\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = math.exp(log_likelihood)\n",
    "\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example II: perplexity for BERT models\n",
    "\n",
    "BERT is a bidirectional model trained with a masked language model (MLM) objective, hence the calculation of its perplexity is different from that of GPT2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.399293899536133\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Input text with masked tokens\n",
    "text = \"The capital of France is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits\n",
    "\n",
    "# Get the predicted probabilities for the masked token\n",
    "masked_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "predicted_probs = torch.softmax(predictions[0, masked_index], dim=-1)\n",
    "\n",
    "# Calculate log probabilities for the actual token\n",
    "actual_token = tokenizer.convert_tokens_to_ids(\"paris\")  # Example actual token\n",
    "log_probs = torch.log(predicted_probs[:, actual_token])\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = torch.exp(-log_probs.mean()).item()\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example III: perplexity for ChatGPT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def get_response_and_perplexity(text, model=\"gpt-3.5-turbo\"):\n",
    "\n",
    "    # Prepare messages for the chat completion\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": text},\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        max_tokens=100,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5,\n",
    "    )\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "\n",
    "    # extract log probabilities\n",
    "    logprobs = response.choices[0].logprobs.content\n",
    "\n",
    "    # calculate token-level probabilities\n",
    "    token_logprobs = []\n",
    "    for token_info in logprobs:\n",
    "        if token_info.top_logprobs:\n",
    "            next_token = token_info.token\n",
    "            for lp in token_info.top_logprobs:\n",
    "                # extract the probabilities of SELECTED token\n",
    "                if lp.token == next_token:\n",
    "                    token_logprobs.append(lp.logprob)\n",
    "                    break\n",
    "            else:\n",
    "                token_logprobs.append(max(lp.logprob for lp in token_info.top_logprobs))\n",
    "\n",
    "    # calculate cross-entropy\n",
    "    cross_entropy = -np.mean(token_logprobs)\n",
    "\n",
    "    # calculate perplexity\n",
    "    perplexity = np.exp2(cross_entropy)\n",
    "\n",
    "    return response_text, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: I'm not familiar with the term \"self-RAG.\" It could be a specific acronym or term used in a particular context. If you provide more information or context, I may be able to help you better.\n",
      "Perplexity: 1.177751014395226\n"
     ]
    }
   ],
   "source": [
    "test_text = \"what is self-RAG\"\n",
    "response, perplexity = get_response_and_perplexity(test_text)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Superman is a fictional superhero appearing in American comic books published by DC Comics. He was created by writer Jerry Siegel and artist Joe Shuster and first appeared in Action Comics #1 in 1938. Superman is known for his superhuman abilities, including super strength, flight, invulnerability, heat vision, and more. He is also known as Clark Kent, a journalist for the Daily Planet in the fictional city of Metropolis.\n",
      "Perplexity: 1.0679336836927034\n"
     ]
    }
   ],
   "source": [
    "test_text = \"who is superman\"\n",
    "response, perplexity = get_response_and_perplexity(test_text)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 9.11 is larger than 9.9.\n",
      "Perplexity: 1.0097724289290273\n"
     ]
    }
   ],
   "source": [
    "test_text = \"which is larger, 9.11 or 9,9?\"\n",
    "response, perplexity = get_response_and_perplexity(test_text)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 9.9 is larger than 9.11.\n",
      "Perplexity: 1.00103644208786\n"
     ]
    }
   ],
   "source": [
    "test_text = \"which is larger, 9.11 or 9.9?\"\n",
    "response, perplexity = get_response_and_perplexity(test_text)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There are 2 \"r\"s in the word \"strawberry.\"\n",
      "Perplexity: 1.133197498793775\n"
     ]
    }
   ],
   "source": [
    "test_text = \"how many r are there in strawberry?\"\n",
    "response, perplexity = get_response_and_perplexity(test_text)\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
